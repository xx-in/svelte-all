# 最速下降法、牛顿法和共轭梯度法

这三种方法都是用于求解 **无约束最优化问题**：

$$
\min_{x \in \mathbb{R}^n} f(x)
$$

其中 $f(x)$ 可微，甚至二阶可微。

在解决问题之前，需要先了解以下两个量代表的几何含义：

- 梯度是函数在该点的 “变化率向量”，梯度越大则越陡，梯度越小则越平缓
- 梯度的二阶范数则是表示梯度大小的直接观测量，当梯度范数足够小时，意味着已接近平缓区域

---

## 一、最速下降法（Steepest Descent Method）

在当前点 $x_k$ 处，沿 **负梯度方向**（下降最快方向）搜索：

$$
d_k = - \nabla f(x_k)
$$

其中步长 $\lambda_k > 0$,用 $\epsilon$表示允许的误差精度，当$\|d^{(k)}\| \le \epsilon$时，停止计算。

对于梯度和梯度二阶范数的计算我们已经很熟悉了，但是步长$\lambda_k$是怎么算的？？?

步长指的是将当前函数点移动的距离，是一个数字，为了接近最优点，最速下降法要求沿着负梯度方向一直走到该方向的最小函数点上。其中:

$$
\varphi(\lambda_k) = f(x_k+\lambda_k d_k)
$$

需要计算$\lambda_k$使得函数$\varphi(\lambda_k)$取得最小值，也就是：

$$
\varphi'(\lambda_k) = 0
$$

通过上式计算出步长后，我们就能获得一个新的点$x_{k+1} = x_k + \lambda_k d_k$，然后开始下一次计算。

> 该算法比较符合直观理解，但对于实际问题，收敛速度可能非常慢。

---

## 二、牛顿法（Newton’s Method）

该方法则在最速下降法的基础上进行了改进，利用 **泰勒二阶展开** 近似目标函数：

$$
f(x) \approx f(x_k) + \nabla f(x_k)^T (x - x_k)
+ \frac{1}{2} (x - x_k)^T \nabla^2 f(x_k) (x - x_k)
$$

令梯度为 0，得到更新方向：

$$
d_k = - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)
$$

这里步长不用计算，默认为$1$，更新公式：

$$
x_{k+1} = x_k + d_k = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)
$$

牛顿法解题会出现三种情况：

- 收敛到极小点
- 收敛到鞍点
- 海森矩阵不可逆，无法继续

所以牛顿法的初始点非常重要。

> 牛顿法不用计算步长，计算出方向就是下一个点！！！。

---

## 三、 共轭梯度法（Conjugate Gradient Method）

共轭梯度解标准二次型函数，

其中$g_k = \nabla f(x_k) $,只是一个符号替换，两个符号是相等的，

选择第一个起始点，任意点记为$x_1$，后续的第k个迭代点记为$x_k$。

同最速下降法一样，第一个点的方向$d_1 = -g_k$，通过精确的一维搜索计算出步长$\lambda_1$。

此时下一个迭代点$x_2$有：

$$
x_2 = x_1 + d_1 \lambda_1
$$

最大的区别就是下一个方向的计算：

$$
d_2 = \frac{\|g_2\|^2}{\|g_1\|^2}d_1 - g_2
$$

---

## 四、总结

其中 $g_k = \nabla f(x_k) $

- 最速下降法

  $$
  d_k  = -\nabla f(x_k)
  $$

$$
  \varphi'(\lambda_k) = 0 \Rightarrow \lambda_k
$$

- 牛顿法

$$
d_k = -[\nabla^2 f(x_k)]^{-1}\nabla f(x_k)
$$

$$
\lambda_k = 1
$$

- 共轭梯度法

$$
d_1 =  - g_1
$$

$$
d_k = \frac{\|g_k\|^2}{\|g_{k-1}\|^2}d_{k-1} - g_k
$$
