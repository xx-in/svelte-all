# 凸优化复习

## 一、 凸集、凸函数和凸规划

### （1）凸集（Convex Set）

一个集合 $C \subseteq \mathbb{R}^n$ 被称为 **凸集**，  
如果对于任意的 $x_1, x_2 \in C$ 和任意满足 $0 \le \theta \le 1$ 的实数 $\theta$，都有：

$$
\theta x_1 + (1 - \theta) x_2 \in C
$$

也就是说，集合中任意两点连成的线段仍然完全包含在该集合中。

### （2）凸函数（Convex Function）

定义在凸集 $C \subseteq \mathbb{R}^n$ 上的函数  
$f: C \to \mathbb{R}$ 被称为 **凸函数**，  
如果对于任意的 $x_1, x_2 \in C$ 和 $0 \le \theta \le 1$，都有：

$$
f(\theta x_1 + (1 - \theta) x_2)
\le
\theta f(x_1) + (1 - \theta) f(x_2)
$$

若等号仅在 $x_1 = x_2$ 时成立，则称 $f$ 为 **严格凸函数**。

### （3） 凸规划（Convex Optimization / Convex Programming）

一个 **凸规划问题** 是指目标函数和可行域都为凸的优化问题。  
其标准形式为：

$$
\min_{x \in \mathbb{R}^n} \; f_0(x)
$$

$$
\text{s.t.} \quad f_i(x) \le 0, \quad i = 1, 2, \dots, m
$$

$$
A x = b
$$

其中：

- $f_0, f_1, \dots, f_m$ 为凸函数；
- 约束集合 $\{x \mid f_i(x) \le 0, A x = b\}$ 为凸集；
- 因此整个问题是一个 **凸优化问题**。

> 💡 若目标函数为凸函数且可行域为凸集， 则 **任意局部最优解都是全局最优解**。

### （4）凸组合（Convex Combination）

给定向量空间 $\mathbb{R}^n$ 中的点 $x_1, x_2, \dots, x_k$，  
若存在一组实数 $\theta_1, \theta_2, \dots, \theta_k$，满足：

$$
\theta_i \ge 0, \quad i = 1, 2, \dots, k
$$

$$
\sum_{i=1}^{k} \theta_i = 1
$$

则称点

$$
x = \sum_{i=1}^{k} \theta_i x_i
$$

为这些点 $x_1, x_2, \dots, x_k$ 的 **凸组合（convex combination）**。

换句话说，凸组合是对若干点的加权平均，其中权重非负且权重之和为 1。

### （5） 超平面（Hyperplane）

在向量空间 $\mathbb{R}^n$ 中，称集合

$$
H = \{ x \in \mathbb{R}^n \mid a^T x = b \}
$$

为一个 **超平面（hyperplane）**，  
其中 $a \in \mathbb{R}^n$ 是非零向量（法向量），$b \in \mathbb{R}$ 是常数。

- 超平面是 $\mathbb{R}^n$ 中维度为 $n-1$ 的平面。
- 超平面将空间分成两个半空间：

$$
H_+ = \{ x \mid a^T x \ge b \}, \quad H_- = \{ x \mid a^T x \le b \}
$$

- 两个半空间在欧式空间$R^n$上也是凸集，并且规定空集$\emptyset$也是凸集

### （6）超平面是凸集的证明

要证明 $H$ 是凸集，需证明：  
对任意 $x_1, x_2 \in H$，以及任意 $\theta \in [0,1]$，都有：

$$
\theta x_1 + (1-\theta)x_2 \in H
$$

**证明：**

1. 由于 $x_1, x_2 \in H$，所以

$$
a^T x_1 = b, \quad a^T x_2 = b
$$

2. 考虑凸组合 $\theta x_1 + (1-\theta)x_2$：

$$
a^T (\theta x_1 + (1-\theta)x_2)
= \theta a^T x_1 + (1-\theta) a^T x_2
= \theta b + (1-\theta)b
= b
$$

3. 因此

$$
\theta x_1 + (1-\theta)x_2 \in H
$$

所以 $H$ 是凸集。

> 💡 直观理解：超平面是“高维空间中的平面”，  
> 任意两点连线仍在超平面上，因此它是凸的，也是凹的。

### （7） 凸函数的性质

设 $f: C \to \mathbb{R}$ 是定义在凸集 $C \subseteq \mathbb{R}^n$ 上的凸函数。

1. 局部最优解也是全局最优解

若 $x^* \in C$ 是 $f$ 的局部最小点，则 $x^*$ 也是全局最小点：

$$
f(x^*) \le f(x), \quad \forall x \in C
$$

2. 一阶条件（可微函数）若 $f$ 可微，则 $f$ 是凸函数当且仅当：

$$
f(y) \ge f(x) + \nabla f(x)^T (y - x), \quad \forall x, y \in C
$$

> 直观：凸函数图形在每一点都位于该点切线的上方。

3. 二阶条件（$C^2$ 可微函数）若 $f$ 二阶可微，则 $f$ 是凸函数当且仅当其 Hessian 半正定：

$$
\nabla^2 f(x) \succeq 0, \quad \forall x \in C
$$

若 Hessian **严格正定**，则 $f$ 为严格凸函数：

$$
\nabla^2 f(x) \succ 0, \quad \forall x \in C
$$

4. 线性组合若 $f_1, f_2, \dots, f_m$ 是凸函数，且 $\alpha_i \ge 0$，则：

$$
f(x) = \sum_{i=1}^m \alpha_i f_i(x)
$$

也是凸函数。

5. 最大值保持凸性若 $f_1, f_2, \dots, f_m$ 是凸函数，则：

$$
f(x) = \max \{ f_1(x), f_2(x), \dots, f_m(x) \}
$$

也是凸函数。

### （8）凸函数的判定

1. 一阶判定（可微函数）若 $f$ 可微，则 $f$ 是凸函数当且仅当：

$$
f(y) \ge f(x) + \nabla f(x)^T (y - x), \quad \forall x, y \in C
$$

> 直观：函数图像在每一点的切平面（梯度线）下方。

2. 二阶判定（$C^2$ 可微函数）若 $f$ 二阶可微，则 $f$ 是凸函数当且仅当 Hessian 半正定：

$$
\nabla^2 f(x) \succeq 0, \quad \forall x \in C
$$

若 Hessian **严格正定**，则 $f$ 是严格凸函数：

$$
\nabla^2 f(x) \succ 0, \quad \forall x \in C
$$

> 注：判定时通常需要采用定义法或者二阶判定法，因为一阶判定存在任意点的要求，在计算上不好操作。

### （9）半正定矩阵

矩阵 $$A$$ 是半正定矩阵，当且仅当：

$$
x^T A x \ge 0, \quad \forall x \in \mathbb{R}^n
$$

判断方式，通常使用特征值判断：

$$
  A \succeq 0 \iff \text{所有特征值 } \lambda_i \ge 0
$$

使用特征值法考虑矩阵：

$$
A =
\begin{bmatrix}
2 & -1 & 0 \\
-1 & 2 & -1 \\
0 & -1 & 2
\end{bmatrix}
$$

写出特征方程，特征值 $\lambda$ 满足：

$$
\det(A - \lambda I) = 0
$$

$$
A - \lambda I =
\begin{bmatrix}
2-\lambda & -1 & 0 \\
-1 & 2-\lambda & -1 \\
0 & -1 & 2-\lambda
\end{bmatrix}
$$

计算行列式，三阶行列式展开：

$$
\det(A-\lambda I) =
(2-\lambda)
\begin{vmatrix} 2-\lambda & -1 \\ -1 & 2-\lambda \end{vmatrix}
- (-1)
\begin{vmatrix} -1 & -1 \\ 0 & 2-\lambda \end{vmatrix}
+ 0
$$

计算每一部分：

1. 二阶行列式：

$$
\begin{vmatrix} 2-\lambda & -1 \\ -1 & 2-\lambda \end{vmatrix} = (2-\lambda)^2 - 1
$$

2. 第二项：

$$
-(-1) \cdot \begin{vmatrix}-1 & -1 \\ 0 & 2-\lambda\end{vmatrix} = -(2-\lambda)
$$

所以总行列式：

$$
\det(A-\lambda I) = (2-\lambda)((2-\lambda)^2 - 1) - (2-\lambda) = (2-\lambda)((2-\lambda)^2 - 2)
$$

解特征方程

$$
(2-\lambda)((2-\lambda)^2 - 2) = 0
$$

解得：

$$
\lambda_1 = 2, \quad \lambda_2 = 2 - \sqrt{2}, \quad \lambda_3 = 2 + \sqrt{2}
$$

三个特征值都大于零，所以是半正定矩阵（事实上是正定矩阵）。

## 二、 梯度和Hessian矩阵

### （1） 梯度（Gradient）

设函数 $f: \mathbb{R}^n \to \mathbb{R}$ 可微，在点 $x = (x_1, x_2, \dots, x_n)^T$ 处的 **梯度**
定义为：

$$

\nabla f(x) = \begin{bmatrix} \frac{\partial f}{\partial x_1} (x) \\\\ \frac{\partial f}{\partial
x_2} (x) \\\\ \vdots \\\\ \frac{\partial f}{\partial x_n} (x) \end{bmatrix} \in \mathbb{R}^n


$$

- 梯度是一个列向量。
- 指向函数在该点上升最快的方向。
- 梯度的模长表示函数在该方向的变化速率。

### （2） 海森矩阵（Hessian Matrix）

设函数 $f: \mathbb{R}^n \to \mathbb{R}$ 二阶可微，在点 $x \in \mathbb{R}^n$ 处的 **海森矩阵**
定义为：

$$

\nabla^2 f(x) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial
x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\\\ \frac{\partial^2
f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2
f}{\partial x_2 \partial x_n} \\\\ \vdots & \vdots & \ddots & \vdots \\\\ \frac{\partial^2
f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots &
\frac{\partial^2 f}{\partial x_n^2} \end{bmatrix} \in \mathbb{R}^{n \times n}


$$

- 海森矩阵是对函数的二阶偏导数的矩阵。
- 对称矩阵（若 $f$ 的二阶偏导连续）。
- 海森矩阵用于判断函数的凸性：
  - 若 $\nabla^2 f(x) \succeq 0$，则 $f$ 在 $x$ 附近凸。
  - 若 $\nabla^2 f(x) \succ 0$，则 $f$ 在 $x$ 附近严格凸。

常用海森矩阵计算：

$$

f(x)=\frac{1}{2}x^TAx + b^Tx + c


$$

$$

\nabla f(x) = Ax + b


$$

$$

\nabla^2 f(x) = A


$$

## 三、单纯形法（Simplex Method）

单纯形法是一种用于求解 **线性规划问题（Linear Programming, LP）** 的经典算法。

### （1）标准形式

线性规划的标准形式为：

$$

\begin{aligned} \max\_{x} \quad & c^T x \\ \text{s.t.} \quad & A x = b, \\ & x \ge 0 \end{aligned}


$$

其中：

- $x \in \mathbb{R}^n$ 为决策变量；
- $A \in \mathbb{R}^{m \times n}$；
- $b \in \mathbb{R}^m$；
- $c \in \mathbb{R}^n$。

### （2）基本思想

- 线性规划的可行域是一个 **凸多面体（convex polyhedron）**；
- 最优解一定位于多面体的一个 **顶点（vertex）** 上；
- 单纯形法沿着多面体的边（约束交点）在顶点之间移动，使目标函数单调增加，直到最优。

### （3）基本变量与非基本变量

将约束 $A x = b$ 中的变量分为：

- **基本变量（basic variables）**：对应 $A$ 的一个可逆子矩阵 $B$；
- **非基本变量（nonbasic variables）**：其余变量。

设：

$$

x = \begin{bmatrix} x_B \\ x_N \end{bmatrix}, \quad A = [B \; N]


$$

则：

$$

B x_B + N x_N = b \quad \Rightarrow \quad x_B = B^{-1} b - B^{-1} N x_N


$$

当 $x_N = 0$ 时，可得一个 **基本可行解（BFS）**：

$$

x_B = B^{-1} b, \quad x_N = 0


$$

### （4）检验最优性

目标函数为：

$$

z = c_B^T x_B + c_N^T x_N


$$

代入 $x_B = B^{-1} b - B^{-1} N x_N$ 得：

$$

z = c_B^T B^{-1} b + (c_N^T - c_B^T B^{-1} N)x_N


$$

定义 **检验数（reduced cost）**：

$$

\bar{c}\_N^T = c_N^T - c_B^T B^{-1} N


$$

- 若所有 $\bar{c}_N \le 0$（最大化问题），则当前解为最优；
- 否则，选择 $\bar{c}_j > 0$ 的变量进入基。

### （5）换基操作（Pivot）

1. **入基变量（Entering Variable）**：选取 $\bar{c}_j > 0$ 的非基本变量 $x_j$。

2. **出基变量（Leaving Variable）**：为保持 $x_B \ge 0$，计算：

$$

\theta_i = \frac{(B^{-1} b)\_i}{(B^{-1} N_j)\_i}, \quad (B^{-1} N_j)\_i > 0


$$

取最小比率：

$$

\theta = \min_i \theta_i


$$

对应的 $x_{B_i}$ 出基。

3. **更新基矩阵**：用 $x_j$ 替换 $x_{B_i}$，得到新的 $B$，继续迭代。

### （6）终止条件

- 若所有 $\bar{c}_N \le 0$，则当前解最优；
- 若存在 $(B^{-1} N_j) \le 0$，则问题无界；
- 若 $b < 0$ 无可行解，可用“大 $M$ 法”或“两阶段法”求初始可行基。

### （7）示例

求：

$$

\max z = 3x_1 + 2x_2


$$

约束：

$$

\begin{cases} x_1 + x_2 \le 4 \\ x_1 \le 2 \\ x_2 \le 3 \\ x_1, x_2 \ge 0 \end{cases}


$$

引入松弛变量：

$$

\begin{aligned} x_1 + x_2 + s_1 &= 4 \\ x_1 + s_2 &= 2 \\ x_2 + s_3 &= 3 \end{aligned}


$$

初始基：$B = [s_1, s_2, s_3]$

初始解：

$$

x_1 = 0, \; x_2 = 0, \; s_1 = 4, \; s_2 = 2, \; s_3 = 3


$$

通过单纯形表迭代，最终得到最优解：

$$

x*1 = 1, \; x_2 = 3, \; z*{\max} = 9


$$

## 四、凸集分离定理、Farkas引理、Gorden 定理

这三条定理揭示了 **凸性、线性不等式可行性** 与 **对偶性** 之间的深刻联系。

### （1）凸集分离定理（Separation Theorem）

设 $C_1, C_2 \subseteq \mathbb{R}^n$ 为两个不相交的 **凸集**，即：

$$

C_1 \cap C_2 = \emptyset


$$

则存在一个非零向量 $a \in \mathbb{R}^n$ 和实数 $\alpha$，使得：

$$

a^T x \le \alpha, \quad \forall x \in C_1


$$

$$

a^T y \ge \alpha, \quad \forall y \in C_2


$$

即存在一个 **超平面** $H = \{x \mid a^T x = \alpha\}$ 可以**将两个凸集严格或弱地分开**。

- 超平面 $H$ 是两凸集之间的“分界线”；
- 若其中一个集合是闭的，另一个是紧的，则可实现**严格分离**。

### （2） Farkas 引理（Farkas' Lemma）

Farkas 引理是线性不等式系统可行性的基本判定定理。

给定矩阵 $A \in \mathbb{R}^{m \times n}$ 和向量
$b \in \mathbb{R}^m$，以下两个系统中，必有且仅有一个可行：

1. 存在 $x \in \mathbb{R}^n$，使得：

$$

A x = b, \quad x \ge 0


$$

2. 存在 $y \in \mathbb{R}^m$，使得：$$

A^T y \ge 0, \quad b^T y < 0

$$

- 系统 (1)：原始可行性（primal feasibility）；
- 系统 (2)：对偶证伪性（dual infeasibility certificate）。

即：

> 若线性方程 $A x = b, \; x \ge 0$ 无解，
> 则存在一个 $y$，使得 $A^T y \ge 0$ 且 $b^T y < 0$。

- $A x = b, x \ge 0$ 表示 $b$ 是 $A$ 的非负列向量的凸组合；
- 若 $b$ 不在该锥中，则存在一个超平面（由 $y$ 定义）将它分开。

即 **Farkas 引理是凸集分离定理在线性锥上的特例**。

### （3）Gordan 定理（Gordan's Theorem）

Gordan 定理是 Farkas 引理的一个“齐次版本”。

给定 $A \in \mathbb{R}^{m \times n}$，以下两个系统不可能同时有解：

1. 存在 $x \in \mathbb{R}^n$，使得：


$$

A x > 0

$$

2. 存在 $y \in \mathbb{R}^m$，使得：
$$

A^T y = 0, \quad y \ge 0, \; y \ne 0

$$

- (1) 表示存在一个 $x$ 使得所有线性形式 $a_i^T x > 0$；
- (2) 表示存在一个非负向量 $y$ 使得它是 $A$ 的零向量；
- 两者不可能同时成立。

Gordan 定理可以看作 Farkas 引理的特例：


$$

b = 0

$$

时，Farkas 引理退化为 Gordan 定理。

| 定理         | 形式     | 关键词            | 几何含义                                  |
| :-- | :- | :- | :- |
| 凸集分离定理 | 一般凸集 | 超平面分离        | 任意不相交凸集可被线性函数分开            |
| Farkas 引理  | 线性系统 | 可行性/对偶性     | 一个系统无解 ⇔ 存在对偶证伪               |
| Gordan 定理  | 齐次系统 | 线性代数版 Farkas | $Ax > 0$ 与 $A^T y = 0, y \ge 0$ 不可共存 |

## 五、线性规划的对偶理论

线性规划（Linear Programming, LP）对偶理论揭示了 **原始问题（Primal）** 与 **对偶问题（Dual）**
之间的对称关系。
它是优化理论与经济学解释的基础。

### （1）标准形式（Primal Problem）

线性规划的标准形式为：


$$

\begin{aligned} \min\_{x \in \mathbb{R}^n} \quad & c^T x \\ \text{s.t.} \quad & A x = b, \\ & x \ge
0, \end{aligned}

$$

其中：

- $A \in \mathbb{R}^{m \times n}$，
- $b \in \mathbb{R}^m$，
- $c \in \mathbb{R}^n$。



### （2）对偶问题（Dual Problem）

构造拉格朗日函数：


$$

L(x, y, s) = c^T x + y^T (b - A x) - s^T x, \quad s \ge 0,

$$

其中：

- $y \in \mathbb{R}^m$ 为等式约束的拉格朗日乘子；
- $s \ge 0$ 为非负约束的乘子。

令对偶函数为：


$$

g(y, s) = \inf\_{x \ge 0} L(x, y, s)

$$

要使 $g(y, s)$ 有界，需要满足：


$$

\nabla_x L = c - A^T y - s = 0

$$

即：


$$

A^T y + s = c, \quad s \ge 0

$$

将其代回得：


$$

g(y, s) = b^T y

$$

于是得到对偶问题：


$$

\begin{aligned} \max\_{y \in \mathbb{R}^m} \quad & b^T y \\ \text{s.t.} \quad & A^T y \le c
\end{aligned}

$$



### （3）原始与对偶的对称关系

| 原始问题 (Primal) | 对偶问题 (Dual)      |
| :- | :- |
| $\min c^T x$      | $\max b^T y$         |
| $A x = b$         | $A^T y \le c$        |
| $x \ge 0$         | ——（对应不等式约束） |

这种一一对应称为 **对偶映射规则**。



### （4）弱对偶定理（Weak Duality）

若 $x$ 是原始可行解，$y$ 是对偶可行解，则有：


$$

c^T x \ge b^T y

$$

**证明：**

由 $A^T y \le c$ 且 $x \ge 0$ 可得：


$$

b^T y = y^T A x \le c^T x

$$

因此：


$$

\text{对偶的目标值始终不超过原始的目标值。}

$$



### （5）强对偶定理（Strong Duality）

若原始问题存在最优解且可行域非空，则对偶问题也存在最优解，且两者最优值相等：


$$

\min*{x \ge 0, A x = b} c^T x = \max*{A^T y \le c} b^T y

$$

即：


$$

c^T x^_ = b^T y^_

$$



### （6）互补松弛条件（Complementary Slackness）

在最优点 $(x^*, y^*, s^*)$ 处，满足：


$$

s*i^* x*i^* = 0, \quad \forall i = 1, \dots, n

$$

几何意义：

- 若 $x_i^* > 0$，则其对应的不等式约束是**紧的**（$A^T y = c$ 在该分量上成立）；
- 若 $A^T y_i < c_i$，则对应的 $x_i^* = 0$。

这描述了原始与对偶变量间的“零积关系”。

## 六、最优性条件

KKT 条件（Karush–Kuhn–Tucker Conditions）是带约束优化中判断最优解的核心工具。
它不仅描述了一阶平衡，还能通过二阶信息判断候选点是否最优或鞍点。



### （1）KKT 条件的含义

- KKT 条件是一组代数方程与不等式，用于判断点 $x^*$ 是否可能是最优解（候选点）。
- 它结合了：
  1. **原始约束**（Primal feasibility）
  2. **对偶约束**（Dual feasibility）
  3. **互补松弛**（Complementary slackness）
  4. **梯度平衡条件**（Stationarity）



### （2）KKT 条件的一般形式

对于问题：


$$

\min\_{x} \quad f(x), \quad \text{s.t. } g_i(x) \le 0, \ i=1,\dots,m, \quad h_j(x) = 0, \
j=1,\dots,p,

$$

引入拉格朗日乘子 $\lambda_i \ge 0$（不等式）与 $\mu_j$（等式），KKT 条件为：


$$

\begin{cases} \nabla f(x^_) + \sum\_{i=1}^m \lambda_i^_ \nabla g*i(x^\*) + \sum*{j=1}^p \mu*j^*
\nabla h*j(x^*) = 0, & \text{(Stationarity)} \\ g*i(x^*) \le 0, \quad h*j(x^*) = 0, &
\text{(Feasibility)} \\ \lambda*i^* \ge 0, & \text{(Dual feasibility)} \\ \lambda*i^* g_i(x^\*) = 0,
& \text{(Complementary slackness)} \end{cases}

$$



### （3）线性规划中的 KKT 条件

对于线性规划：


$$

\min_x \quad c^T x, \quad \text{s.t. } A x = b, \ x \ge 0,

$$

KKT 条件为：


$$

\begin{cases} A x^_ = b, & x^_ \ge 0, \\ A^T y^_ + s^_ = c, & s^_ \ge 0, \\ s_i^_ x_i^\* = 0, &
\forall i. \end{cases}

$$

其中：

- $y$ 是等式约束的拉格朗日乘子；
- $s$ 是非负约束对应的对偶变量；
- $s_i x_i = 0$ 表示互补松弛。



### （4）KKT 条件的计算步骤

1. 写出拉格朗日函数：


$$

L(x, \lambda, \mu) = f(x) + \sum_i \lambda_i g_i(x) + \sum_j \mu_j h_j(x)

$$

2. 计算一阶偏导：


$$

\nabla_x L(x, \lambda, \mu) = 0

$$

3. 代入约束条件：


$$

g_i(x) \le 0, \quad h_j(x) = 0

$$

4. 添加互补松弛条件：


$$

\lambda_i g_i(x) = 0, \quad \lambda_i \ge 0

$$

5. 联立方程求解，得到候选点 $(x^*, \lambda^*, \mu^*)$。



### （5）判断 KKT 点是否最优或鞍点

1. **凸优化问题**
- 若目标函数 $f$ 和不等式约束 $g_i$ 均为凸函数，等式约束 $h_j$ 为线性函数，
  则任何满足 KKT 条件的点 $x^*$ **必为全局最优**。

2. **非凸问题**
- 构造拉格朗日 Hessian：
$$

     H = \nabla^2_{x x} L(x^*, \lambda^*, \mu^*)
     $$

- 检查 **可行方向上的二阶曲率**：
  $$
  \mathcal{D}(x^*) = \{ d \mid \nabla h_j(x^*)^T d = 0, \ \nabla g_i(x^*)^T d \le 0 \text{ for active } i \}
  $$
  若 $d^T H d \ge 0$ 对所有 $d \in \mathcal{D}(x^*)$，则 $x^*$ 为局部极小点；  
  若 $H$ 不定，则 $x^*$ 为鞍点；若负定，则为局部极大点。

3. **直观理解**
   - KKT 条件保证一阶梯度平衡：没有沿可行方向立即下降的趋势；
   - Hessian 检查决定二阶曲率：向上 → 极小，向下 → 极大，正负混合 → 鞍点。

### （6）小结

- KKT 条件提供候选最优点；
- **凸性** 可确保 KKT 点为全局最优；
- **Hessian 正定性** 可区分局部极小点与鞍点；
- 对线性规划，KKT 条件与对偶理论完全等价。

## 七、非线性规划的拉格朗日对偶理论

非线性规划（Nonlinear Programming,
NLP）的拉格朗日对偶理论，是线性规划对偶理论的推广，用于带约束的凸或非凸优化问题。

### （1）原始问题（Primal Problem）

设非线性规划为：

$$
\begin{aligned}
\min_{x \in \mathbb{R}^n} \quad & f(x) \\
\text{s.t.} \quad & g_i(x) \le 0, \ i=1,\dots,m, \\
& h_j(x) = 0, \ j=1,\dots,p,
\end{aligned}
$$

其中：

- $f(x)$ 为目标函数；
- $g_i(x)$ 为不等式约束；
- $h_j(x)$ 为等式约束。

### （2）拉格朗日函数（Lagrangian）

引入拉格朗日乘子：

- $\lambda_i \ge 0$ 对应不等式约束；
- $\mu_j$ 对应等式约束。

构造拉格朗日函数：

$$
L(x, \lambda, \mu) = f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^p \mu_j h_j(x)
$$

### （3）拉格朗日对偶函数（Dual Function）

定义对偶函数：

$$
g(\lambda, \mu) = \inf_{x} L(x, \lambda, \mu)
$$

- $g(\lambda, \mu)$ 对任意 $\lambda \ge 0$ 是 **下界函数**：
  $$
  g(\lambda, \mu) \le f(x), \quad \forall x \text{ 可行}.
  $$

### （4）对偶问题（Dual Problem）

对偶问题为：

$$
\begin{aligned}
\max_{\lambda \ge 0, \mu} \quad & g(\lambda, \mu)
\end{aligned}
$$

- 对偶问题总是提供原始问题的 **下界**（最小化问题）；
- 若原问题是凸问题，且满足 **Slater 条件**，则强对偶成立：
  $$
  \min f(x) = \max g(\lambda, \mu)
  $$

### （5）KKT 条件与对偶最优性

在凸问题下，原始可行点 $x^*$ 和对偶可行点 $(\lambda^*, \mu^*)$ 同时满足：

$$
\begin{cases}
\nabla_x L(x^*, \lambda^*, \mu^*) = 0, \\
g_i(x^*) \le 0, \ h_j(x^*) = 0, \\
\lambda_i^* \ge 0, \\
\lambda_i^* g_i(x^*) = 0,
\end{cases}
$$

则 $(x^*, \lambda^*, \mu^*)$ 为 **原始最优解和对偶最优解**。

### （6）判别最优性的方法

1. **凸问题**：
   - 只需检查 KKT 条件，满足即为全局最优。
2. **非凸问题**：
   - 计算 Hessian：
     $$
     H = \nabla^2_{xx} L(x^*, \lambda^*, \mu^*)
     $$
   - 检查可行方向上的曲率：
     - 半正定 → 局部极小点
     - 不定 → 鞍点
     - 负定 → 局部极大点

### （7）计算步骤总结

1. 写出拉格朗日函数 $L(x, \lambda, \mu)$。
2. 求梯度并联立 $\nabla_x L = 0$。
3. 代入原始约束 $g_i(x) \le 0, h_j(x) = 0$。
4. 添加互补松弛条件 $\lambda_i g_i(x) = 0, \lambda_i \ge 0$。
5. 联立求解得到候选 $(x^*, \lambda^*, \mu^*)$。
6. 若凸问题 → KKT 点即全局最优；若非凸 → 检查 Hessian 判别鞍点或局部极小值。

### （8）直观理解

- 拉格朗日对偶理论把 **约束问题转化为无约束的下界优化问题**；
- KKT 条件是原问题与对偶问题同时最优的桥梁；
- 对凸问题，KKT 点 + Slater 条件保证全局最优；
- 对非凸问题，需要二阶信息（Hessian）判断局部最优或鞍点。

## 八、 最速下降法、牛顿法和共轭梯度法

这三种方法都是用于求解 **无约束最优化问题**：

$$
\min_{x \in \mathbb{R}^n} f(x)
$$

其中 $f(x)$ 可微，甚至二阶可微。

### （1）最速下降法（Steepest Descent Method）

在当前点 $x_k$ 处，沿 **负梯度方向**（下降最快方向）搜索：

$$
d_k = - \nabla f(x_k)
$$

更新公式：

$$
x_{k+1} = x_k + \alpha_k d_k = x_k - \alpha_k \nabla f(x_k)
$$

其中步长 $\alpha_k > 0$ 通过线搜索（line search）确定：

$$
\alpha_k = \arg \min_{\alpha > 0} f(x_k - \alpha \nabla f(x_k))
$$

- 简单易实现；
- 收敛速度慢，尤其在条件数很大的问题中；
- 每次仅使用梯度信息。

### （2）牛顿法（Newton’s Method）

利用 **泰勒二阶展开** 近似目标函数：

$$
f(x) \approx f(x_k) + \nabla f(x_k)^T (x - x_k)
+ \frac{1}{2} (x - x_k)^T \nabla^2 f(x_k) (x - x_k)
$$

令梯度为 0，得到更新方向：

$$
d_k = - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)
$$

更新公式：

$$
x_{k+1} = x_k + d_k = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)
$$

- 收敛速度：局部二次收敛；
- 需要计算和求逆海森矩阵；
- 计算代价高，但对凸二次函数收敛很快。

### （3） 共轭梯度法（Conjugate Gradient Method）

用于求解 **二次优化问题** 或 **大型稀疏对称正定线性方程组**：

$$
f(x) = \frac{1}{2} x^T A x - b^T x, \quad A \succ 0
$$

等价于求解：

$$
A x = b
$$

构造一系列 **A-共轭方向** $\{d_k\}$，在每个方向上一次到达最优点。

初始化：

$$
x_0 \text{ 给定}, \quad r_0 = b - A x_0, \quad d_0 = r_0
$$

迭代：

$$
\alpha_k = \frac{r_k^T r_k}{d_k^T A d_k}, \quad
x_{k+1} = x_k + \alpha_k d_k
$$

更新残差和方向：

$$
r_{k+1} = r_k - \alpha_k A d_k, \quad
\beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}, \quad
d_{k+1} = r_{k+1} + \beta_k d_k
$$

- 不需要存储或求逆矩阵；
- 理论上 $n$ 步收敛（精确算术下）；
- 实际中非常高效，适合大规模问题。

### （4）方法比较

|    方法    | 方向                                   | 收敛速度 | 是否用海森矩阵 | 适用规模     |
| :--------: | :------------------------------------- | :------- | :------------- | :----------- |
| 最速下降法 | $-\nabla f(x_k)$                       | 线性     | 否             | 小型         |
|   牛顿法   | $-[\nabla^2 f(x_k)]^{-1}\nabla f(x_k)$ | 二次     | 是             | 小型到中型   |
| 共轭梯度法 | A-共轭方向                             | 超线性   | 只需矩阵乘法   | 大型稀疏系统 |

## 九、罚函数法

罚函数法（Penalty Method）是求解约束优化问题的一类数值方法，  
通过将约束“吸收”到目标函数中，将约束优化问题转化为一系列无约束优化问题。

### （1）原理

给定约束优化问题：

$$
\min f(x), \quad
\text{s.t. } g_i(x) \le 0, \ i=1,\dots,m, \quad
h_j(x) = 0, \ j=1,\dots,p
$$

罚函数法构造**罚函数** $P(x, r)$，将违反约束的代价加到目标中：

$$
F(x, r) = f(x) + P(x, r)
$$

其中 $r > 0$ 为**罚因子**，控制约束违背的惩罚强度。

### （2）常用罚函数类型

1. **外罚函数（Exterior Penalty）**
   - 对违反约束的点施加惩罚，函数在可行域外定义：
     $$
     P(x, r) = r \left( \sum_{i=1}^m \max\{0, g_i(x)\}^2 + \sum_{j=1}^p h_j(x)^2 \right)
     $$

2. **内罚函数（Interior/Barrier Penalty）**
   - 在可行域内部定义，靠近约束边界罚值趋于无穷：
     $$
     P(x, r) = -\frac{1}{r} \sum_{i=1}^m \ln(-g_i(x))
     $$

- 外罚函数适合逐步逼近可行解；
- 内罚函数用于保持迭代点严格在可行域内。

### （3）算法步骤（外罚函数法示例）

1. 选取初始罚因子 $r_0 > 0$ 和初始点 $x_0$。
2. 构造无约束函数：

   $$
   F(x, r_k) = f(x) + r_k \sum_{i=1}^m \max\{0, g_i(x)\}^2 + r_k \sum_{j=1}^p h_j(x)^2
   $$

3. 求解无约束优化：

   $$
   x_{k+1} = \arg\min F(x, r_k)
   $$

4. 增加罚因子 $r_{k+1} > r_k$，使约束惩罚更大。
5. 重复步骤 2–4，直到约束满足精度 $\epsilon$。

### （4）收敛性说明

- 对凸问题，外罚函数法可保证 $x_k \to x^*$，逼近原问题可行最优解；
- 对非凸问题，需要选择足够大的罚因子 $r_k$，同时注意避免数值病态（罚因子过大导致优化困难）。

### （5）优缺点

**优点**：

- 简化约束问题为无约束问题；
- 易于与梯度法、牛顿法等结合。

**缺点**：

- 大罚因子可能导致函数条件数变差；
- 内外罚函数选择和参数调整需要经验。

### （6）直观理解

- 外罚函数：约束被“罚”出来，迭代逐步靠近可行域边界；
- 内罚函数：约束被“障碍”挡住，迭代始终在可行域内部；
- 罚函数法通过不断调整罚因子，使无约束最小点趋向原约束最优点。

## 十、收敛速率

## 十、收敛速率

收敛速率（Convergence Rate）是评价优化算法效率的重要指标，描述迭代序列 $x_k$ 接近最优点 $x^*$
的速度。

### （1）收敛定义

设迭代序列 $\{x_k\}$ 收敛到 $x^*$，即：

$$
\lim_{k \to \infty} x_k = x^*
$$

定义误差序列：

$$
e_k = \|x_k - x^*\|
$$

### （2）线性收敛（Linear Convergence）

若存在常数 $0 < \rho < 1$，使得：

$$
e_{k+1} \le \rho e_k, \quad k \text{ 足够大}
$$

则称迭代序列 **线性收敛**。

- 误差按比例衰减；
- 收敛速度与 $\rho$ 成反比，$\rho$ 越小越快。

### （3）超线性收敛（Superlinear Convergence）

若：

$$
\lim_{k \to \infty} \frac{e_{k+1}}{e_k} = 0
$$

则称 **超线性收敛**，即误差衰减速度越来越快。

- 牛顿法在目标函数光滑且 Hessian 非奇异时通常是超线性收敛。

### （4）二次收敛（Quadratic Convergence）

若存在 $M > 0$，使得：

$$
e_{k+1} \le M e_k^2, \quad k \text{ 足够大}
$$

则称 **二次收敛**。

- 收敛速度非常快，每次迭代误差大约平方级减小；
- 牛顿法在无约束凸优化问题满足条件时可达到二次收敛。

### （5）常用判别方法

1. 计算误差序列 $e_k$，检查其衰减规律；
2. 绘制 $\log(e_k)$ 或 $\log(e_{k+1}/e_k)$ 对迭代次数曲线；
3. 根据斜率判断收敛类别：
   - 线性：固定负斜率
   - 超线性：斜率逐渐变陡
   - 二次：误差平方级下降

### （6）收敛速率的意义

- 指导算法选择：高阶方法（如牛顿法）适合对精度要求高的问题；
- 对罚函数法、梯度下降法等迭代方法，可评估参数设置（步长、罚因子）对收敛速度的影响；
- 在实际计算中，线性收敛速度慢，需要调节迭代策略；二次收敛效率高，但每步计算代价大。
