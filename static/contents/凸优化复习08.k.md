# 最速下降法、牛顿法和共轭梯度法

这三种方法都是用于求解 **无约束最优化问题**：

$$
\min_{x \in \mathbb{R}^n} f(x)
$$

其中 $f(x)$ 可微，甚至二阶可微。

## 一、最速下降法（Steepest Descent Method）

在当前点 $x_k$ 处，沿 **负梯度方向**（下降最快方向）搜索：

$$
d_k = - \nabla f(x_k)
$$

更新公式：

$$
x_{k+1} = x_k + \alpha_k d_k = x_k - \alpha_k \nabla f(x_k)
$$

其中步长 $\alpha_k > 0$ 通过线搜索（line search）确定：

$$
\alpha_k = \arg \min_{\lambda > 0} f(x_k - \alpha \nabla f(x_k))
$$

若 $\epsilon$表示允许的误差精度，当$\|d^{(k)}\| \le \epsilon$时，停止计算。

## 二、牛顿法（Newton’s Method）

利用 **泰勒二阶展开** 近似目标函数：

$$
f(x) \approx f(x_k) + \nabla f(x_k)^T (x - x_k)
+ \frac{1}{2} (x - x_k)^T \nabla^2 f(x_k) (x - x_k)
$$

令梯度为 0，得到更新方向：

$$
d_k = - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)
$$

更新公式：

$$
x_{k+1} = x_k + d_k = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)
$$

- 收敛速度：局部二次收敛；
- 需要计算和求逆海森矩阵；
- 计算代价高，但对凸二次函数收敛很快。

## 三、 共轭梯度法（Conjugate Gradient Method）

用于求解 **二次优化问题** 或 **大型稀疏对称正定线性方程组**：

$$
f(x) = \frac{1}{2} x^T A x - b^T x, \quad A \succ 0
$$

等价于求解：

$$
A x = b
$$

构造一系列 **A-共轭方向** $\{d_k\}$，在每个方向上一次到达最优点。

初始化：

$$
x_0 \text{ 给定}, \quad r_0 = b - A x_0, \quad d_0 = r_0
$$

迭代：

$$
\alpha_k = \frac{r_k^T r_k}{d_k^T A d_k}, \quad
x_{k+1} = x_k + \alpha_k d_k
$$

更新残差和方向：

$$
r_{k+1} = r_k - \alpha_k A d_k, \quad
\beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}, \quad
d_{k+1} = r_{k+1} + \beta_k d_k
$$

- 不需要存储或求逆矩阵；
- 理论上 $n$ 步收敛（精确算术下）；
- 实际中非常高效，适合大规模问题。

## 四、方法比较

|    方法    | 方向                                   | 收敛速度 | 是否用海森矩阵 | 适用规模     |
| :--------: | :------------------------------------- | :------- | :------------- | :----------- |
| 最速下降法 | $-\nabla f(x_k)$                       | 线性     | 否             | 小型         |
|   牛顿法   | $-[\nabla^2 f(x_k)]^{-1}\nabla f(x_k)$ | 二次     | 是             | 小型到中型   |
| 共轭梯度法 | A-共轭方向                             | 超线性   | 只需矩阵乘法   | 大型稀疏系统 |
